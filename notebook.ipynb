{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f805740",
   "metadata": {},
   "source": [
    "# Development Notebook for Career Pages Watchdog    \n",
    "\n",
    "This file is meant to be able to test out functionalities before production.\n",
    "\n",
    "The system is made of the following parts:\n",
    "\n",
    "1. Career pages extractor:  Using selenium and LLMs for a given company URL we will auto-detect it's career page url where all jobs can be found.\n",
    "2. Job count extractor: Given a career page, we will try to estimate how many jobs they have published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82a56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver as uc\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "nw_openai_api_key = os.getenv('NW_OPENAI_API_KEY')\n",
    "if nw_openai_api_key is None:\n",
    "    raise ValueError(\"NW OpenAI API key is not set. Please check your .env file or environment variables.\")\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"OpenAI API key is not set. Please check your .env file or environment variables.\")\n",
    "\n",
    "MODEL_QWEN = 'qwen2.5'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "MODEL_NW_GPT = 'gpt-4o-mini'\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "nwopenai = OpenAI(base_url='https://litellm.data-int.odyssey.preview.nwse.cloud', api_key=nw_openai_api_key)\n",
    "\n",
    "system_prompt_career_page_finder = \"\"\"You will act as a web scraper and search fot the page that contains the list of job openings for a given company. \\\n",
    "Bear in mind, that you'll be looking at german websites, so the page might be in german. \\\n",
    "Some keyword to look for are \"Careers\", \"Jobs\", \"Join Us\", \"Work with us\", \"Karriere\", \"stellenangebote\", or similar. \\\n",
    "Many sites also have their jobs inside Career or Karriere section. \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3f6d6",
   "metadata": {},
   "source": [
    "## Test Qwen running from local\n",
    "\n",
    "Be sure to run\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(\n",
    "            model=MODEL_QWEN,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "                {\"role\": \"user\", \"content\": \"Hallo qwen, wie geht es dir?\"},])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46f61f",
   "metadata": {},
   "source": [
    "## Scrapper functions\n",
    "\n",
    "Now let's add the website scrapper and link extractor functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0de2fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def order_links_user_prompt(links):\n",
    "        user_prompt = (\n",
    "        \"You are analyzing a list of links from a company's website. \"\n",
    "        \"Your task is to rank them from most likely to least likely to point to the company's job listings page — \"\n",
    "        \"the page where users can actively browse or search for open positions.\\n\\n\"\n",
    "        \"Links that contain terms like 'job-search', 'stellenangebote', 'search-jobs', or similar phrases should be ranked \"\n",
    "        \"higher than more generic pages like 'karriere', 'career', or 'about'. The goal is to find the actual **job search interface** — \"\n",
    "        \"a page that likely has job filters, job counters, and application buttons.\\n\\n\"\n",
    "        \"Note: the site may be in German. Prioritize links that clearly suggest access to a list of current job openings.\\n\\n\"\n",
    "        \"Here are the links (some may be relative):\\n\"\n",
    "        )\n",
    "        user_prompt += \"\\n\".join(links)\n",
    "        user_prompt += \"\\n\\nRespond only in JSON format like this:\\n\"\n",
    "        user_prompt += \"\"\"\n",
    "        {\n",
    "        \"links\": [\n",
    "            \"https://full.url/job-search\",\n",
    "            \"https://full.url/karriere\"\n",
    "        ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        return user_prompt\n",
    "\n",
    "def check_job_page_user_prompt(content):\n",
    "    user_prompt = (\n",
    "        \"You are analyzing the content of a subpage from a company's website. \"\n",
    "        \"Your task is to determine whether this specific page is where the company lists its job openings.\\n\\n\"\n",
    "        \"Indicators that the page lists job openings include:\\n\"\n",
    "        \"- A list of current job openings with titles and descriptions\\n\"\n",
    "        \"- A job search interface with filters (e.g., location, department)\\n\"\n",
    "        \"- Buttons or links labeled 'Apply Now' or similar\\n\"\n",
    "        \"- A job counter indicating the number of available positions (e.g., '123 jobs available')\\n\\n\"\n",
    "        \"Note that general career pages that provide information about working at the company, company culture, or employee testimonials, \"\n",
    "        \"but do not list specific job openings, should not be considered as pages that list job openings.\\n\\n\"\n",
    "        \"Content might be in German.\\n\\n\"\n",
    "        \"Only the career page with apply button without search interface or job listings is not a job listing page.\\n\\n\"\n",
    "        \"If you find a job search interface or/and a job counter like '123 jobs available', please answer with **yes**. \"\n",
    "        \"Please answer only with yes or no.\\n\\n\"\n",
    "        f\"Here is the page content:\\n---\\n{content[:7000]}\\n---\"\n",
    "    )\n",
    "    return user_prompt\n",
    "\n",
    "def order_links_llm(links, openaiSession, model=MODEL_QWEN):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "            {\"role\": \"user\", \"content\": order_links_user_prompt(links)}\n",
    "        ]\n",
    "        response = openaiSession.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        # Parse the JSON response to a Python dictionary\n",
    "        result_dict = json.loads(result)\n",
    "        # Extract the list of links from the dictionary\n",
    "        ordered_links = result_dict[\"links\"]\n",
    "        return ordered_links\n",
    "\n",
    "def check_job_page_llm(content, openaiSession, model=MODEL_QWEN):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "            {\"role\": \"user\", \"content\": check_job_page_user_prompt(content)}\n",
    "        ]\n",
    "        response = openaiSession.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip().lower()\n",
    "        # print(f\"LLM response: {result}\")\n",
    "        # Check if the response contains \"yes\" or \"no\"\n",
    "        return result == \"yes\"\n",
    "\n",
    "def crawl(driver, base_url, url, collected_links, all_seen_links=None, max_depth=2, current_depth=0, openaiSession=None, model=MODEL_LLAMA):\n",
    "        # Initialize the all_seen_links set if it's None\n",
    "        if all_seen_links is None:\n",
    "            all_seen_links = set()\n",
    "\n",
    "        if current_depth > max_depth:\n",
    "            return\n",
    "        \n",
    "        if url in collected_links:\n",
    "            return\n",
    "        \n",
    "        # Add the current URL to our collected links\n",
    "        collected_links.add(url)\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(random.uniform(1.5, 3.5))  # Human-like pause\n",
    "            \n",
    "            print(f\"Crawling (depth {current_depth}): {url}\")\n",
    "\n",
    "            if current_depth != 0:\n",
    "                page_source = driver.page_source\n",
    "                # Parse the page source with BeautifulSoup\n",
    "                soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "                # Remove irrelevant tags to clean up the content\n",
    "                for irrelevant in soup([\"script\", \"style\", \"img\", \"input\", \"meta\", \"noscript\", \"iframe\"]):\n",
    "                    irrelevant.decompose()\n",
    "\n",
    "                # Extract visible text from the page\n",
    "                page_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "                # Further clean up the text by removing excessive newlines\n",
    "                page_text = \"\\n\".join(line for line in page_text.splitlines() if line.strip())\n",
    "\n",
    "                # Check if the page is a job listings page using the LLM\n",
    "                if check_job_page_llm(page_text, openaiSession, model):\n",
    "                    print(f\"✅ {url} is a job listings page.\")\n",
    "                    return\n",
    "                else:\n",
    "                    print(f\"❌ {url} is not the job listing page.\")\n",
    "\n",
    "            # Find all <a> links\n",
    "            hrefs = []\n",
    "            links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            for link in links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                # Only include links that:\n",
    "                # 1. Are not None\n",
    "                # 2. Start with the base URL (same domain)\n",
    "                # 3. Haven't been seen before across all pages\n",
    "                if href and href.startswith(base_url) and href not in all_seen_links:  \n",
    "                    hrefs.append(href)\n",
    "                    # Add to the all_seen_links set to avoid duplicate exploration\n",
    "                    all_seen_links.add(href)\n",
    "\n",
    "            print(f\"Found {len(hrefs)} new links in {url}\")\n",
    "            if len(hrefs) > 0:\n",
    "                print(\"New links:\")\n",
    "                for link in hrefs[:3]:  # Only show first 5 to avoid console clutter\n",
    "                    print(link) \n",
    "\n",
    "                if openaiSession:\n",
    "                    try:\n",
    "                        ordered_hrefs = order_links_llm(hrefs, openaiSession, model)\n",
    "                        print(\"Ordered links:\")\n",
    "                        for link in ordered_hrefs[:3]:  # Only show first 5\n",
    "                            print(link) \n",
    "                        # Now recursively crawl those hrefs\n",
    "                        for href in ordered_hrefs[:3]:  # Limit to first 5 to avoid excessive crawling\n",
    "                            crawl(driver, base_url, href, collected_links, all_seen_links, max_depth, current_depth + 1, openaiSession, model)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error ordering links: {e}\")\n",
    "                        # If ordering fails, just use the original links\n",
    "                        for href in hrefs[:3]:  # Limit to first 5\n",
    "                            crawl(driver, base_url, href, collected_links, all_seen_links, max_depth, current_depth + 1, openaiSession, model)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to crawl {url}: {e}\")\n",
    "\n",
    "def get_career_page(url, openaiSession, model=MODEL_LLAMA):\n",
    "    \"\"\"\n",
    "    Create this WebsiteSelenium object from the given URL using Selenium and BeautifulSoup.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract base URL for domain matching\n",
    "    base_url_parts = url.split('/')\n",
    "    if len(base_url_parts) >= 3:\n",
    "        base_url = base_url_parts[0] + '//' + base_url_parts[2]\n",
    "    else:\n",
    "        base_url = url\n",
    "    \n",
    "    print(f\"Processing website: {url}\")\n",
    "    print(f\"Base domain: {base_url}\")\n",
    "    \n",
    "    # Create a fresh driver for each website\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    # More human-like user agent\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\")\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    # Try with regular Chrome driver if undetected fails\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = uc.Chrome(options=options)\n",
    "        print(\"Browser initialized successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create undetected_chromedriver: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Configure browser to avoid detection\n",
    "        driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "            \"source\": \"\"\"\n",
    "                Object.defineProperty(navigator, 'webdriver', {\n",
    "                    get: () => undefined\n",
    "                })\n",
    "            \"\"\"\n",
    "        })\n",
    "        \n",
    "        collected_links = set()\n",
    "        all_seen_links = set()  # Initialize the set of all seen links\n",
    "        max_depth = 2  # Keep depth low for testing\n",
    "        \n",
    "        # Start crawling\n",
    "        try:\n",
    "            crawl(driver, base_url, url, collected_links, all_seen_links, max_depth, 0, openaiSession, model)\n",
    "            \n",
    "            return url  # Return original URL if no links were found\n",
    "        except Exception as e:\n",
    "            print(f\"Error during crawling: {e}\")\n",
    "            return url\n",
    "    finally:\n",
    "        try:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "                print(\"Browser closed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error closing driver: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e41729",
   "metadata": {},
   "source": [
    "## Now let's look at the companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c03230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Company:\n",
    "    def __init__(self, company_name, company_url):\n",
    "        \"\"\"\n",
    "        Initialize a Company object with the given name and URL.\n",
    "\n",
    "        Args:\n",
    "            company_name (str): The name of the company.\n",
    "            company_url (str): The URL of the company's website.\n",
    "        \"\"\"\n",
    "        self.company_name = company_name\n",
    "        self.company_url = company_url\n",
    "        self.career_url = None  # This will be populated later\n",
    "        \n",
    "def process_companies_from_csv(file_path, openaiSession, model=MODEL_LLAMA):\n",
    "    \"\"\"\n",
    "    Process companies from a CSV file, extract their career page URLs, and return a list of Company objects.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing company data.\n",
    "        model (str): The model to use for extracting career page URLs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Company objects with career URLs populated.\n",
    "    \"\"\"\n",
    "    companies = []\n",
    "\n",
    "    # Open the CSV file and iterate over its rows\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)  # Use DictReader to access columns by name\n",
    "        for row in reader:\n",
    "            # Create a Company object for each row\n",
    "            company = Company(company_name=row['company_name'], company_url=row['company_url'])\n",
    "            company.career_url = get_career_page(company.company_url, openaiSession, model)  # Get the career page URL\n",
    "            print(f\"Company Name: {company.company_name}, Career URL: {company.career_url}\")\n",
    "            companies.append(company)  # Add the Company object to the companies list\n",
    "\n",
    "    return companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "490ae36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing website: https://www.munichre.com\n",
      "Base domain: https://www.munichre.com\n",
      "Browser initialized successfully\n",
      "Crawling (depth 0): https://www.munichre.com\n",
      "Found 72 new links in https://www.munichre.com\n",
      "New links:\n",
      "https://www.munichre.com/en.html\n",
      "https://www.munichre.com/en/risks.html\n",
      "https://www.munichre.com/en/solutions.html\n",
      "Ordered links:\n",
      "https://www.munichre.com/en/careers.html\n",
      "Crawling (depth 1): https://www.munichre.com/en/careers.html\n",
      "✅ https://www.munichre.com/en/careers.html is a job listings page.\n",
      "Browser closed successfully\n",
      "Company Name: Munich Re, Career URL: https://www.munichre.com\n",
      "Processing website: https://www.rwe.com\n",
      "Base domain: https://www.rwe.com\n",
      "Browser initialized successfully\n",
      "Crawling (depth 0): https://www.rwe.com\n",
      "Found 189 new links in https://www.rwe.com\n",
      "New links:\n",
      "https://www.rwe.com/en/contact-services/\n",
      "https://www.rwe.com/en/contact-services/apps-and-tools/\n",
      "https://www.rwe.com/en/\n",
      "Ordered links:\n",
      "https://www.rwe.com/en/rwe-careers-portal/job-offers/\n",
      "https://www.rwe.com/en/rwe-careers-portal/\n",
      "https://www.rwe.com/en/rwe-careers-portal/faq/\n",
      "Crawling (depth 1): https://www.rwe.com/en/rwe-careers-portal/job-offers/\n",
      "❌ https://www.rwe.com/en/rwe-careers-portal/job-offers/ is not the job listing page.\n",
      "Found 0 new links in https://www.rwe.com/en/rwe-careers-portal/job-offers/\n",
      "Crawling (depth 1): https://www.rwe.com/en/rwe-careers-portal/\n",
      "❌ https://www.rwe.com/en/rwe-careers-portal/ is not the job listing page.\n",
      "Found 0 new links in https://www.rwe.com/en/rwe-careers-portal/\n",
      "Browser closed successfully\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprocess_companies_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/sampledomains.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenai\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_GPT\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mprocess_companies_from_csv\u001b[39m\u001b[34m(file_path, openaiSession, model)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Create a Company object for each row\u001b[39;00m\n\u001b[32m     32\u001b[39m     company = Company(company_name=row[\u001b[33m'\u001b[39m\u001b[33mcompany_name\u001b[39m\u001b[33m'\u001b[39m], company_url=row[\u001b[33m'\u001b[39m\u001b[33mcompany_url\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     company.career_url = \u001b[43mget_career_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompany_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenaiSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get the career page URL\u001b[39;00m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCompany Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany.company_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Career URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany.career_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m     companies.append(company)  \u001b[38;5;66;03m# Add the Company object to the companies list\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 204\u001b[39m, in \u001b[36mget_career_page\u001b[39m\u001b[34m(url, openaiSession, model)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# Start crawling\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollected_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_seen_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenaiSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m url  \u001b[38;5;66;03m# Return original URL if no links were found\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 146\u001b[39m, in \u001b[36mcrawl\u001b[39m\u001b[34m(driver, base_url, url, collected_links, all_seen_links, max_depth, current_depth, openaiSession, model)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# Now recursively crawl those hrefs\u001b[39;00m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m href \u001b[38;5;129;01min\u001b[39;00m ordered_hrefs[:\u001b[32m3\u001b[39m]:  \u001b[38;5;66;03m# Limit to first 5 to avoid excessive crawling\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollected_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_seen_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenaiSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError ordering links: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mcrawl\u001b[39m\u001b[34m(driver, base_url, url, collected_links, all_seen_links, max_depth, current_depth, openaiSession, model)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     91\u001b[39m     driver.get(url)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Human-like pause\u001b[39;00m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCrawling (depth \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m current_depth != \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', openai, MODEL_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158898c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', nwopenai, MODEL_NW_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process without browser automation\n",
    "process_companies_from_csv('data/sampledomains.csv', openai, MODEL_GPT, use_browser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4356edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careerpageswatchdog-Xx4QWZVo-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
