{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f805740",
   "metadata": {},
   "source": [
    "# Development Notebook for Career Pages Watchdog    \n",
    "\n",
    "This file is meant to be able to test out functionalities before production.\n",
    "\n",
    "The system is made of the following parts:\n",
    "\n",
    "1. Career pages extractor:  Using selenium and LLMs for a given company URL we will auto-detect it's career page url where all jobs can be found.\n",
    "2. Job count extractor: Given a career page, we will try to estimate how many jobs they have published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "nw_openai_api_key = os.getenv('NW_OPENAI_API_KEY')\n",
    "if nw_openai_api_key is None:\n",
    "    raise ValueError(\"NW OpenAI API key is not set. Please check your .env file or environment variables.\")\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"OpenAI API key is not set. Please check your .env file or environment variables.\")\n",
    "\n",
    "MODEL_QWEN = 'qwen2.5'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "MODEL_NW_GPT = 'gpt-4o-mini'\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "nwopenai = OpenAI(base_url='https://litellm.data-int.odyssey.preview.nwse.cloud', api_key=nw_openai_api_key)\n",
    "\n",
    "system_prompt_career_page_finder = \"\"\"You will act as a web scraper and search fot the page that contains the list of job openings for a given company. \\\n",
    "Bear in mind, that you'll be looking at german websites, so the page might be in german. \\\n",
    "Some keyword to look for are \"Careers\", \"Jobs\", \"Join Us\", \"Work with us\", \"Karriere\", \"stellenangebote\", or similar. \\\n",
    "Many sites also have their jobs inside Career or Karriere section. \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3f6d6",
   "metadata": {},
   "source": [
    "## Test Qwen running from local\n",
    "\n",
    "Be sure to run\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(\n",
    "            model=MODEL_QWEN,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "                {\"role\": \"user\", \"content\": \"Hallo qwen, wie geht es dir?\"},])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46f61f",
   "metadata": {},
   "source": [
    "## Scrapper functions\n",
    "\n",
    "Now let's add the website scrapper and link extractor functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def order_links_user_prompt(links):\n",
    "        user_prompt = f\"You'll analyse a list of links for a company website. \"\n",
    "        user_prompt += \"Now you need to order them. The links that are most likely the career page or job listing page that could contain a job opening counter will be first in the list. Order them from most likely to less likely. Bear in mind the site might be in german.\\n\"\n",
    "        user_prompt += \"Links (some could be relative):\"\n",
    "        user_prompt += \"\\n\".join(links)\n",
    "        user_prompt += \"You should respond in JSON as in this example:\"\n",
    "        user_prompt += \"\"\"\n",
    "        {\n",
    "            \"links\": [\n",
    "                \"https://full.url/goes/here/about\",\n",
    "                \"https://another.full.url/careers\"\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        return user_prompt\n",
    "\n",
    "def check_job_page_user_prompt(content):\n",
    "        user_prompt = (\n",
    "        \"You are analyzing the content of a subpage from a company's website. \"\n",
    "        \"Your task is to determine whether this specific page is where the company lists its job openings.\\n\\n\"\n",
    "        \"Things to look for include:\\n\"\n",
    "        \"- Phrases like 'open positions', 'jobs', 'career opportunities', etc. (in English or German)\\n\"\n",
    "        \"- Buttons or links to apply for jobs\\n\"\n",
    "        \"- Job search inputs, filters, or department/category selectors\\n\"\n",
    "        \"- A job counter (e.g., '123 jobs available')\\n\\n\"\n",
    "        \"Content might be in German.\\n\\n\"\n",
    "        \"Please answer with a single word: **yes** or **no**.\\n\\n\"\n",
    "        f\"Here is the page content:\\n---\\n{content[:3500]}\\n---\"  # Trim to avoid prompt length issues\n",
    "        )   \n",
    "\n",
    "def order_links_llm(links, openaiSession, model=MODEL_QWEN):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "            {\"role\": \"user\", \"content\": order_links_user_prompt(links)}\n",
    "        ]\n",
    "        response = openaiSession.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        # Parse the JSON response to a Python dictionary\n",
    "        result_dict = json.loads(result)\n",
    "        # Extract the list of links from the dictionary\n",
    "        ordered_links = result_dict[\"links\"]\n",
    "        return ordered_links\n",
    "\n",
    "def check_job_page_llm(url, openaiSession, model=MODEL_QWEN):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "            {\"role\": \"user\", \"content\": check_job_page_user_prompt(url)}\n",
    "        ]\n",
    "        response = openaiSession.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip().lower()\n",
    "        return result == \"yes\"\n",
    "\n",
    "def crawl(driver, base_url, url, collected_links, max_depth=2, current_depth=0, openaiSession=None, model=MODEL_LLAMA):\n",
    "        if current_depth > max_depth:\n",
    "            return\n",
    "        \n",
    "        if url in collected_links:\n",
    "            return\n",
    "        \n",
    "        collected_links.add(url)\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(random.uniform(1.5, 3.5))  # Human-like pause\n",
    "            \n",
    "            print(f\"Crawling (depth {current_depth}): {url}\")\n",
    "\n",
    "            if current_depth != 0:\n",
    "                page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "                if check_job_page_user_prompt(page_text):\n",
    "                    print(f\"✅ {url} is a job listings page.\")\n",
    "                    return\n",
    "                else:\n",
    "                    print(f\"❌ {url} is not the job listing page.\")\n",
    "\n",
    "            # Find all <a> links\n",
    "            hrefs = []\n",
    "            links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            for link in links:\n",
    "                href = link.get_attribute(\"href\")\n",
    "                if href and href.startswith(base_url):  # Only crawl inside the same domain\n",
    "                    hrefs.append(href)\n",
    "\n",
    "            print(f\"Found {len(hrefs)} links\")\n",
    "            if len(hrefs) > 0:\n",
    "                print(f\"Sample: {hrefs[:3]}\")\n",
    "\n",
    "                if openaiSession:\n",
    "                    try:\n",
    "                        ordered_hrefs = order_links_llm(hrefs, openaiSession, model)\n",
    "                        print(f\"Ordered links: {ordered_hrefs[:3]}\")\n",
    "                        \n",
    "                        # Now recursively crawl those hrefs\n",
    "                        for href in ordered_hrefs[:5]:  # Limit to first 5 to avoid excessive crawling\n",
    "                            crawl(driver, base_url, href, collected_links, max_depth, current_depth + 1, openaiSession, model)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error ordering links: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to crawl {url}: {e}\")\n",
    "\n",
    "def get_career_page(url, openaiSession, model=MODEL_LLAMA):\n",
    "    \"\"\"\n",
    "    Create this WebsiteSelenium object from the given URL using Selenium and BeautifulSoup.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract base URL for domain matching\n",
    "    base_url_parts = url.split('/')\n",
    "    if len(base_url_parts) >= 3:\n",
    "        base_url = base_url_parts[0] + '//' + base_url_parts[2]\n",
    "    else:\n",
    "        base_url = url\n",
    "    \n",
    "    print(f\"Base URL for domain matching: {base_url}\")\n",
    "    \n",
    "    # Create a fresh driver for each website\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    # More human-like user agent\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\")\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    # Try with regular Chrome driver if undetected fails\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = uc.Chrome(options=options)\n",
    "        print(f\"Successfully created undetected_chromedriver for {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create undetected_chromedriver: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Configure browser to avoid detection\n",
    "        driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "            \"source\": \"\"\"\n",
    "                Object.defineProperty(navigator, 'webdriver', {\n",
    "                    get: () => undefined\n",
    "                })\n",
    "            \"\"\"\n",
    "        })\n",
    "        \n",
    "        collected_links = set()\n",
    "        max_depth = 1  # Keep depth low for testing\n",
    "        \n",
    "        # Start crawling\n",
    "        try:\n",
    "            crawl(driver, base_url, url, collected_links, max_depth, 0, openaiSession, model)\n",
    "            \n",
    "            return url  # Return original URL if no links were found\n",
    "        except Exception as e:\n",
    "            print(f\"Error during crawling: {e}\")\n",
    "            return url\n",
    "    finally:\n",
    "        try:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error closing driver: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5449f3",
   "metadata": {},
   "source": [
    "## Simple URL Guesser Alternative\n",
    "\n",
    "If the browser automation approach is failing, we can try a direct URL guess approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2534f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_career_url(url, openaiSession, model=MODEL_GPT):\n",
    "    \"\"\"Simple function to guess career URL without browser automation\"\"\"\n",
    "    prompt = f\"Given the company website {url}, predict what their careers/jobs page URL would be. Look for common patterns like /careers, /jobs, /karriere for German sites. Return only the full URL without explanation.\"\n",
    "    \n",
    "    response = openaiSession.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert at guessing company career page URLs.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the URL guesser with a problematic URL\n",
    "print(guess_career_url(\"https://www.sparkasse.de\", openai, MODEL_GPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d2ed1",
   "metadata": {},
   "source": [
    "## Test with one URL\n",
    "\n",
    "A simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_career_page(\"https://www.sparkasse.de\",ollama))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e41729",
   "metadata": {},
   "source": [
    "## Now let's look at the companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Company:\n",
    "    def __init__(self, company_name, company_url):\n",
    "        \"\"\"\n",
    "        Initialize a Company object with the given name and URL.\n",
    "\n",
    "        Args:\n",
    "            company_name (str): The name of the company.\n",
    "            company_url (str): The URL of the company's website.\n",
    "        \"\"\"\n",
    "        self.company_name = company_name\n",
    "        self.company_url = company_url\n",
    "        self.career_url = None  # This will be populated later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c03230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_companies_from_csv(file_path, openaiSession, model=MODEL_LLAMA):\n",
    "    \"\"\"\n",
    "    Process companies from a CSV file, extract their career page URLs, and return a list of Company objects.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing company data.\n",
    "        model (str): The model to use for extracting career page URLs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Company objects with career URLs populated.\n",
    "    \"\"\"\n",
    "    companies = []\n",
    "\n",
    "    # Open the CSV file and iterate over its rows\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)  # Use DictReader to access columns by name\n",
    "        for row in reader:\n",
    "            # Create a Company object for each row\n",
    "            company = Company(company_name=row['company_name'], company_url=row['company_url'])\n",
    "            company.career_url = get_career_page(company.company_url, openaiSession, model)  # Get the career page URL\n",
    "            print(f\"Company Name: {company.company_name}, Career URL: {company.career_url}\")\n",
    "            companies.append(company)  # Add the Company object to the companies list\n",
    "\n",
    "    return companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ae36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', openai, MODEL_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158898c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', nwopenai, MODEL_NW_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process without browser automation\n",
    "process_companies_from_csv('data/sampledomains.csv', openai, MODEL_GPT, use_browser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4356edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careerpageswatchdog-Xx4QWZVo-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
