{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f805740",
   "metadata": {},
   "source": [
    "# Development Notebook for Career Pages Watchdog    \n",
    "\n",
    "This file is meant to be able to test out functionalities before production.\n",
    "\n",
    "The system is made of the following parts:\n",
    "\n",
    "1. Career pages extractor:  Using selenium and LLMs for a given company URL we will auto-detect it's career page url where all jobs can be found.\n",
    "2. Job count extractor: Given a career page, we will try to estimate how many jobs they have published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import undetected_chromedriver as uc\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "nw_openai_api_key = os.getenv('NW_OPENAI_API_KEY')\n",
    "if nw_openai_api_key is None:\n",
    "    raise ValueError(\"NW OpenAI API key is not set. Please check your .env file or environment variables.\")\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"OpenAI API key is not set. Please check your .env file or environment variables.\")\n",
    "\n",
    "MODEL_QWEN = 'qwen2.5'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "MODEL_NW_GPT = 'gpt-4o-mini'\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "CHAR_LIMIT = 20000\n",
    "\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "nwopenai = OpenAI(base_url='https://litellm.data.odyssey.preview.nwse.cloud', api_key=nw_openai_api_key)\n",
    "\n",
    "\n",
    "system_prompt_career_page_finder = \"\"\"You will act as a web scraper and search fot the page that contains the list of job openings for a given company. \\\n",
    "Bear in mind, that you'll be looking at german websites, so the page might be in german. \\\n",
    "Some keyword to look for are \"Careers\", \"Jobs\", \"Join Us\", \"Work with us\", \"Karriere\", \"stellenangebote\", or similar. \\\n",
    "Many sites also have their jobs inside Career or Karriere section. \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3f6d6",
   "metadata": {},
   "source": [
    "## Test Qwen running from local\n",
    "\n",
    "Be sure to run\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(\n",
    "            model=MODEL_QWEN,\n",
    "            messages=[      \n",
    "                {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "                {\"role\": \"user\", \"content\": \"Hallo qwen, wie geht es dir?\"},])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46f61f",
   "metadata": {},
   "source": [
    "## Scrapper functions\n",
    "\n",
    "Now let's add the website scrapper and link extractor functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def order_links_user_prompt(links):\n",
    "        user_prompt = (\n",
    "        \"You are analyzing a list of links from a company's website. \"\n",
    "        \"Your task is to rank them from most likely to least likely to point to the company's job listings page — \"\n",
    "        \"the page where users can actively browse or search for open positions.\\n\\n\"\n",
    "        \"Links that contain terms like 'job-search', 'stellenangebote', 'search-jobs', 'jobs.company_url' or similar phrases should be ranked \"\n",
    "        \"higher than more generic pages like 'karriere', 'career', or 'about'. The goal is to find the actual **job search interface** — \"\n",
    "        \"a page that likely has job filters, job counters, and application buttons.\\n\\n\"\n",
    "        \"Prioritize links without query parameters, such as 'https://jobs.siemens-healthineers.com/careers', over links with query parameters, \"\n",
    "        \"such as 'https://jobs.siemens-healthineers.com/careers?pid=563156115690465' .\\n\\n\"\n",
    "        \"Note: the site may be in German. Prioritize links that clearly suggest access to a list of current job openings.\\n\\n\"\n",
    "        )\n",
    "        user_prompt += \"\\n\\nRespond only in JSON format like this and ensure you rank and return ALL the links you get:\\n\"\n",
    "        user_prompt += \"\"\"\n",
    "        {\n",
    "            \"links\": [\n",
    "                \"https://full.url/job-search\",\n",
    "                \"https://full.url/karriere\",\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        user_prompt += \"Here are the links (some may be relative):\\n\"\n",
    "        user_prompt += \"\\n\".join(links)\n",
    "        return user_prompt\n",
    "\n",
    "def check_job_page_user_prompt(content):\n",
    "    user_prompt_old = (\n",
    "        \"You are analyzing the content of a subpage from a company's website. \"\n",
    "        \"Your task is to determine whether this specific page explicitly lists job openings.\\n\\n\"\n",
    "        \"Stronger indicators that the page lists job openings include:\\n\"\n",
    "        \"- A comprehensive list of current job openings with detailed titles and descriptions\\n\"\n",
    "        \"- A job search interface with specific filters (e.g., location, department) and results displayed\\n\"\n",
    "        \"- Prominent buttons or links labeled 'Apply Now' directly corresponding to job titles\\n\"\n",
    "        \"- A visible job counter explicitly indicating the number of available positions (e.g., '123 jobs available') or similar metrics\\n\\n\"\n",
    "        \"General career pages that solely provide information about company values, work culture, employee experiences, \"\n",
    "        \"or other non-specific content related to employment should not be considered as listing specific job openings.\\n\\n\"\n",
    "        \"Pages that have a single 'Apply Now' button without any accompanying job titles or specific descriptions do not qualify as a job listing page.\\n\\n\"\n",
    "        \"Considerations for the presence of elements like detailed descriptions or categorizations tied directly to job roles.\\n\\n\"\n",
    "        \"If you can pinpoint a job search interface or job counter like '123 jobs available', please answer with yes. \"\n",
    "        \"Specific job titles and descriptions MUST be listed, if such specifics are missing, answer with no.\\n\\n\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        \"Determine if this company subpage explicitly lists job openings.\\n\\n\"\n",
    "        \"Requirements for a Job Listings Page:\\n\"\n",
    "        \"- Lists job titles and descriptions.\\n\"\n",
    "        \"- Shows a job counter like '123 jobs available'.\\n\\n\"\n",
    "        \"Non-Qualifying Pages:\\n\"\n",
    "        \"- General career information without specific job details.\\n\"\n",
    "        \"- Mentions of jobs or current jobs.\\n\"\n",
    "        \"- Single 'Apply Now' button with no job titles or descriptions.\\n\\n\"\n",
    "        \"Answer 'yes' if the page lists specific job titles and descriptions, along with a job counter or if it has at least 3 indicators.\\n\"\n",
    "        \"If you are not sure, no job counter is found or not specifics job descriptions are found, answer 'no'.\"\n",
    "        \"Specific job titles and descriptions MUST be listed, if such specifics are missing, answer with no.\\n\\n\"\n",
    "    )\n",
    "    user_prompt += \"\\n\\nRespond only in JSON format like this:\"\n",
    "    user_prompt += \"\"\"\n",
    "    {\n",
    "        \"job_page\": 1 for \"yes\" or 0 for \"no\",\n",
    "        \"available_jobs\": 123,\n",
    "        \"reason\": \"explanation of the decision\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    user_prompt += f\"Here is the page content:\\n---\\n{content[:CHAR_LIMIT]}\\n---\"\n",
    "    return user_prompt\n",
    "\n",
    "def order_links_llm(links, openaiSession, model=MODEL_QWEN):\n",
    "        user_prompt = order_links_user_prompt(links)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        response = openaiSession.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        result = response.choices[0].message.content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "        # Parse the JSON response to a Python dictionary\n",
    "        result_dict = json.loads(result)\n",
    "       \n",
    "        # Extract the list of links from the dictionary\n",
    "        ordered_links = result_dict[\"links\"]\n",
    "        return ordered_links\n",
    "\n",
    "def check_job_page_llm(content, openaiSession, model=MODEL_QWEN):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "            {\"role\": \"user\", \"content\": check_job_page_user_prompt(content)}\n",
    "        ]\n",
    "\n",
    "        response = openaiSession.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        result = response.choices[0].message.content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "        # Parse the JSON response to a Python dictionary\n",
    "        result_dict = json.loads(result)\n",
    "\n",
    "        # Check if the response contains \"yes\" or \"no\"\n",
    "        return result_dict\n",
    "\n",
    "def remove_cookie_banner(driver):\n",
    "    script = \"\"\"\n",
    "    const banner = document.getElementById('cookiebanner');\n",
    "    if (banner) {\n",
    "        banner.remove();\n",
    "        return true;\n",
    "    }\n",
    "    return false;\n",
    "    \"\"\"\n",
    "    result = driver.execute_script(script)\n",
    "    return result\n",
    "\n",
    "def extract_page_source(driver, url):\n",
    "    \"\"\"\n",
    "    Extract the page source from the given URL using Selenium.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(1.5, 3.5))  # Human-like pause\n",
    "\n",
    "            # Use XPath for case-insensitive ID partial matching\n",
    "        try:\n",
    "            # XPath function to ensure case insensitivity\n",
    "            cookie_accept_button = WebDriverWait(driver, 2.5).until(\n",
    "                EC.element_to_be_clickable((\n",
    "                By.XPATH, \n",
    "                     '//*[contains(translate(@id, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"allowall\") or ' +\n",
    "                    'contains(translate(@id, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"acceptall\") or ' +\n",
    "                    'contains(translate(@id, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"accept-all\") or ' +\n",
    "                    'contains(translate(@id, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"accept\") or ' +\n",
    "                    'contains(translate(@data-testid, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"allowall\") or ' +\n",
    "                    'contains(translate(@data-testid, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"acceptall\") or ' +\n",
    "                    'contains(translate(@data-testid, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"accept-all\") or ' +\n",
    "                    'contains(translate(@data-testid, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"accept\") or ' +\n",
    "                    'contains(translate(@class, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"allowall\") or ' +\n",
    "                    'contains(translate(@class, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"acceptall\") or ' +\n",
    "                    'contains(translate(@class, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"accept-all\") or ' +\n",
    "                    'contains(translate(@class, \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", \"abcdefghijklmnopqrstuvwxyz\"), \"accept\")]'\n",
    "                ))\n",
    "            )   \n",
    "\n",
    "            # Click on the \"Allow All\" button\n",
    "            cookie_accept_button.click()\n",
    "            print(\"Cookie consent with case-insensitive 'allowall' ID accepted.\")\n",
    "\n",
    "        except Exception:\n",
    "            print(\"No acceptable cookie consent button found with 'allowall' in its id, case-insensitive check.\")\n",
    "    \n",
    "        # Allow some time for actions after accepting cookies\n",
    "        WebDriverWait(driver, 3)\n",
    "\n",
    "        try:\n",
    "            # Optionally remove the element\n",
    "            removed = remove_cookie_banner(driver)\n",
    "            print(f\"Banner removed from DOM: {removed}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Banner not found or already removed\")\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "\n",
    "         # Check for iframes and append their content to page_source\n",
    "        iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "        for iframe in iframes:\n",
    "            try:\n",
    "                driver.switch_to.frame(iframe)\n",
    "                page_source += driver.page_source\n",
    "                driver.switch_to.default_content()\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to extract content from iframe: {e}\")\n",
    "\n",
    "                # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Remove irrelevant tags to clean up the content\n",
    "        for irrelevant in soup([\"script\", \"style\", \"img\", \"input\", \"meta\", \"noscript\", \"iframe\"]):\n",
    "            irrelevant.decompose()\n",
    "\n",
    "        # Extract visible text from the page\n",
    "        page_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "        # Further clean up the text by removing excessive newlines\n",
    "        page_text = \"\\n\".join(line for line in page_text.splitlines() if line.strip())\n",
    "\n",
    "        # Store the page source in a file for debugging or further analysis\n",
    "        with open(\"data/page_content.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(page_text[:CHAR_LIMIT])\n",
    "\n",
    "        return page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract page source: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl(driver, base_url, url, collected_links, all_seen_links=None, max_depth=2, current_depth=0, openaiSession=None, model=MODEL_LLAMA, found_job_page=None):\n",
    "    # Initialize the all_seen_links set if it's None\n",
    "    if all_seen_links is None:\n",
    "        all_seen_links = set()\n",
    "\n",
    "    # If a job page has already been found, stop further crawling\n",
    "    if found_job_page[0] is not None:\n",
    "        return\n",
    "\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    if url in collected_links:\n",
    "        return\n",
    "    \n",
    "    # Add the current URL to our collected links\n",
    "    collected_links.add(url)\n",
    "    \n",
    "    try:\n",
    "        # Check if the page is a job listings page using the LLM if we are not at the root level\n",
    "        #if current_depth != 0:\n",
    "        page_text = extract_page_source(driver, url)\n",
    "\n",
    "        # Check if the page source contains indications of Cloudflare or scraping protection\n",
    "        if \"cloudflare\" in page_text.lower() or \"attention required\" in page_text.lower():\n",
    "            print(\"⚠️ Warning: The page might be protected by Cloudflare or other scraping protections.\")\n",
    "            found_job_page[0] = 'https://www.cloudflare.com/blocked'\n",
    "            return\n",
    "        \n",
    "        job_page_results = check_job_page_llm(page_text, openaiSession, model)\n",
    "\n",
    "        # Check if the page is a job listings page using the LLM\n",
    "        if job_page_results.get(\"job_page\") == 1:\n",
    "            print(f\"✅ {url} is a job listings page.\")\n",
    "            print(f\"Reason: {job_page_results.get('reason')}\")\n",
    "            found_job_page[0] = url  # Set the found job page URL\n",
    "            return\n",
    "        else:\n",
    "            print(f\"❌ {url} is not the job listing page.\")\n",
    "            print(f\"Reason: {job_page_results.get('reason')}\")\n",
    "\n",
    "        hrefs = []\n",
    "        # Find all <a> links\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            # Only include links that:\n",
    "            # 1. Are not None\n",
    "            # 2. Start with the base URL (same domain) or contain \".base_url\" in their domain\n",
    "            # 3. Haven't been seen before across all pages\n",
    "            if href and href not in all_seen_links:\n",
    "                hrefs.append(href)\n",
    "                # Add to the all_seen_links set to avoid duplicate exploration\n",
    "                all_seen_links.add(href)\n",
    "\n",
    "        # Find all <button> elements with href attributes\n",
    "        buttons = driver.find_elements(By.TAG_NAME, \"button\")\n",
    "        for button in buttons:\n",
    "            href = button.get_attribute(\"href\")\n",
    "            # Apply the same filtering logic as for <a> links\n",
    "            if href and href not in all_seen_links:\n",
    "                hrefs.append(href)\n",
    "                all_seen_links.add(href)\n",
    "\n",
    "        print(f\"Found {len(hrefs)} new links in {url}\")\n",
    "        if len(hrefs) > 0:\n",
    "            if openaiSession:\n",
    "                try:\n",
    "                    ordered_hrefs = order_links_llm(hrefs, openaiSession, model)\n",
    "                    # Print the first ordered link\n",
    "                    if ordered_hrefs:\n",
    "                        print(f\"LLM returned {len(ordered_hrefs)} ordered links\")\n",
    "                        print(f\"Best bet: {ordered_hrefs[0]}\")\n",
    "                    \n",
    "                    # Now recursively crawl those hrefs\n",
    "                    for href in ordered_hrefs[:3]:  # Limit to first 3 to avoid excessive crawling\n",
    "                        crawl(driver, base_url, href, collected_links, all_seen_links, max_depth, current_depth + 1, openaiSession, model, found_job_page)\n",
    "                        # Stop further crawling if a job page has been found\n",
    "                        if found_job_page[0] is not None:\n",
    "                            return\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error ordering links: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to crawl {url}: {e}\")\n",
    "\n",
    "def init_driver():\n",
    "    # Create a fresh driver for each website\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    # More human-like user agent\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\")\n",
    "    options.add_argument(\"--headless=new\")\n",
    "\n",
    "    # Try with regular Chrome driver if undetected fails\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = uc.Chrome(options=options)\n",
    "\n",
    "        # Configure browser to avoid detection\n",
    "        driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "            \"source\": \"\"\"\n",
    "            Object.defineProperty(navigator, 'webdriver', {\n",
    "                get: () => undefined\n",
    "            })\n",
    "            \"\"\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create undetected_chromedriver: {e}\")\n",
    "\n",
    "    return driver \n",
    "\n",
    "def get_page_content(url):\n",
    "    \"\"\"\n",
    "    Get the page content from the given URL using Selenium and BeautifulSoup.\n",
    "    \"\"\"\n",
    "    driver = init_driver()\n",
    "    page_source = extract_page_source(driver, url)\n",
    "\n",
    "    return page_source\n",
    "\n",
    "def get_career_page(url, openaiSession, model=MODEL_LLAMA):\n",
    "    \"\"\"\n",
    "    Create this WebsiteSelenium object from the given URL using Selenium and BeautifulSoup.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract base URL for domain matching\n",
    "    base_url_parts = url.split('/')\n",
    "    if len(base_url_parts) >= 3:\n",
    "        base_url = base_url_parts[0] + '//' + base_url_parts[2]\n",
    "    else:\n",
    "        base_url = url\n",
    "    \n",
    "    print(f\"Processing website: {url}\") \n",
    "    \n",
    "    # Try with regular Chrome driver if undetected fails\n",
    "    driver = init_driver()\n",
    "\n",
    "    if driver is None:\n",
    "        return\n",
    "\n",
    "    try:       \n",
    "        collected_links = set()\n",
    "        all_seen_links = set()  # Initialize the set of all seen links\n",
    "        max_depth = 2  # Keep depth low for testing\n",
    "        found_job_page = [None]  # Use a mutable object to track the found job page URL\n",
    "        \n",
    "        # Start crawling\n",
    "        try:\n",
    "            crawl(driver, base_url, url, collected_links, all_seen_links, max_depth, 0, openaiSession, model, found_job_page)\n",
    "            \n",
    "            # Return the found job page URL if any, otherwise return the original URL\n",
    "            return found_job_page[0] if found_job_page[0] is not None else url\n",
    "        except Exception as e:\n",
    "            print(f\"Error during crawling: {e}\")\n",
    "            return url\n",
    "    finally:\n",
    "        try:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "                print(\"Browser closed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error closing driver: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e41729",
   "metadata": {},
   "source": [
    "## Now let's look at the companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c03230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Company:\n",
    "    def __init__(self, company_name, company_url):\n",
    "        \"\"\"\n",
    "        Initialize a Company object with the given name and URL.\n",
    "\n",
    "        Args:\n",
    "            company_name (str): The name of the company.\n",
    "            company_url (str): The URL of the company's website.\n",
    "        \"\"\"\n",
    "        self.company_name = company_name\n",
    "        self.company_url = company_url\n",
    "        self.career_url = None  # This will be populated later\n",
    "        \n",
    "def process_companies_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Process companies from a CSV file, extract their career page URLs, and return a list of Company objects.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing company data.\n",
    "        model (str): The model to use for extracting career page URLs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Company objects with career URLs populated.\n",
    "    \"\"\"\n",
    "    companies = []\n",
    "\n",
    "    # Open the CSV file and iterate over its rows\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)  # Use DictReader to access columns by name\n",
    "        for row in reader:\n",
    "            # Create a Company object for each row\n",
    "            company = Company(company_name=row['company_name'], company_url=row['company_url'])\n",
    "            companies.append(company)  # Add the Company object to the companies list\n",
    "\n",
    "    # Save companies to a CSV file\n",
    "    output_file = 'data/processed_companies.csv'\n",
    "    with open(output_file, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Company Name', 'Company URL', 'Career URL'])\n",
    "        # Write the company data\n",
    "        try:\n",
    "            for company in companies:\n",
    "                company.career_url = get_career_page(company.company_url, nwopenai, MODEL_NW_GPT)  # Get the career page URL\n",
    "                writer.writerow([company.company_name, company.company_url, company.career_url])\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing companies: {e}\")\n",
    "            file.close()  # Ensure the file is closed in case of an error\n",
    "            raise\n",
    "\n",
    "    print(f\"Companies saved to {output_file}\")\n",
    "\n",
    "    return companies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc8036",
   "metadata": {},
   "source": [
    "## CHAT GPT OFFICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ae36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', openai, MODEL_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_career_page('https://www.munichre.com', openai, MODEL_GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d99f8",
   "metadata": {},
   "source": [
    "## OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158898c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a187aec",
   "metadata": {},
   "source": [
    "## NW CHAT GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = process_companies_from_csv('data/sampledomains.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df5f76",
   "metadata": {},
   "source": [
    "### Debugging different cases\n",
    "\n",
    "Sometimes the prompt for detecting job listing pages does not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a27196",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = get_page_content('https://www.munichre.com')\n",
    "check_job_page_llm(page_content, nwopenai, MODEL_NW_GPT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careerpageswatchdog-Xx4QWZVo-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
