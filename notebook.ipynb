{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f805740",
   "metadata": {},
   "source": [
    "# Development Notebook for Career Pages Watchdog    \n",
    "\n",
    "This file is meant to be able to test out functionalities before production.\n",
    "\n",
    "The system is made of the following parts:\n",
    "\n",
    "1. Career pages extractor:  Using selenium and LLMs for a given company URL we will auto-detect it's career page url where all jobs can be found.\n",
    "2. Job count extractor: Given a career page, we will try to estimate how many jobs they have published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82a56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver as uc\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "nw_openai_api_key = os.getenv('NW_OPENAI_API_KEY')\n",
    "if nw_openai_api_key is None:\n",
    "    raise ValueError(\"NW OpenAI API key is not set. Please check your .env file or environment variables.\")\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"OpenAI API key is not set. Please check your .env file or environment variables.\")\n",
    "\n",
    "MODEL_QWEN = 'qwen2.5'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "MODEL_NW_GPT = 'gpt-4o-mini'\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "nwopenai = OpenAI(base_url='https://litellm.data.odyssey.preview.nwse.cloud', api_key=nw_openai_api_key)\n",
    "\n",
    "system_prompt_career_page_finder = \"\"\"You will act as a web scraper and search fot the page that contains the list of job openings for a given company. \\\n",
    "Bear in mind, that you'll be looking at german websites, so the page might be in german. \\\n",
    "Some keyword to look for are \"Careers\", \"Jobs\", \"Join Us\", \"Work with us\", \"Karriere\", \"stellenangebote\", or similar. \\\n",
    "Many sites also have their jobs inside Career or Karriere section. \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3f6d6",
   "metadata": {},
   "source": [
    "## Test Qwen running from local\n",
    "\n",
    "Be sure to run\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(\n",
    "            model=MODEL_QWEN,\n",
    "            messages=[      \n",
    "                {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "                {\"role\": \"user\", \"content\": \"Hallo qwen, wie geht es dir?\"},])\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46f61f",
   "metadata": {},
   "source": [
    "## Scrapper functions\n",
    "\n",
    "Now let's add the website scrapper and link extractor functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de2fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def order_links_user_prompt(links):\n",
    "        user_prompt = (\n",
    "        \"You are analyzing a list of links from a company's website. \"\n",
    "        \"Your task is to rank them from most likely to least likely to point to the company's job listings page — \"\n",
    "        \"the page where users can actively browse or search for open positions.\\n\\n\"\n",
    "        \"Links that contain terms like 'job-search', 'stellenangebote', 'search-jobs', or similar phrases should be ranked \"\n",
    "        \"higher than more generic pages like 'karriere', 'career', or 'about'. The goal is to find the actual **job search interface** — \"\n",
    "        \"a page that likely has job filters, job counters, and application buttons.\\n\\n\"\n",
    "        \"Note: the site may be in German. Prioritize links that clearly suggest access to a list of current job openings.\\n\\n\"\n",
    "        \"Here are the links (some may be relative):\\n\"\n",
    "        )\n",
    "        user_prompt += \"\\n\".join(links)\n",
    "        user_prompt += \"\\n\\nRespond only in JSON format like this and ensure you return all the links you get:\\n\"\n",
    "        user_prompt += \"\"\"\n",
    "        {\n",
    "            \"links\": [\n",
    "                \"https://full.url/job-search\",\n",
    "                \"https://full.url/karriere\"\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        return user_prompt\n",
    "\n",
    "def check_job_page_user_prompt(content):\n",
    "    user_prompt = (\n",
    "        \"You are analyzing the content of a subpage from a company's website. \"\n",
    "        \"Your task is to determine whether this specific page explicitly lists job openings.\\n\\n\"\n",
    "        \"Stronger indicators that the page lists job openings include:\\n\"\n",
    "        \"- A comprehensive list of current job openings with detailed titles and descriptions\\n\"\n",
    "        \"- A job search interface with specific filters (e.g., location, department) and results displayed\\n\"\n",
    "        \"- Prominent buttons or links labeled 'Apply Now' directly corresponding to job titles\\n\"\n",
    "        \"- A visible job counter explicitly indicating the number of available positions (e.g., '123 jobs available') or similar metrics\\n\\n\"\n",
    "        \"General career pages that solely provide information about company values, work culture, employee experiences, \"\n",
    "        \"or other non-specific content related to employment should not be considered as listing specific job openings.\\n\\n\"\n",
    "        \"Pages that have a single 'Apply Now' button without any accompanying job titles or specific descriptions do not qualify as a job listing page.\\n\\n\"\n",
    "        \"Considerations for the presence of elements like detailed descriptions or categorizations tied directly to job roles.\\n\\n\"\n",
    "        \"If you can pinpoint a job search interface or job counter like '123 jobs available', please answer with **yes**. \"\n",
    "        \"If such specifics are missing, answer with **no**. Please answer with yes or no and provide a list of offered jobs.\\n\\n\"\n",
    "        f\"Here is the page content:\\n---\\n{content[:7000]}\\n---\"\n",
    "    )\n",
    "    return user_prompt\n",
    "\n",
    "def order_links_llm(links, openaiSession, model=MODEL_QWEN):\n",
    "        print(f\"ASKING LLM TO ORDER {len(links)} LINKS\")\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "            {\"role\": \"user\", \"content\": order_links_user_prompt(links)}\n",
    "        ]\n",
    "        response = openaiSession.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "\n",
    "        print(f\"LLM response: {result}\")\n",
    "        # Parse the JSON response to a Python dictionary\n",
    "        result_dict = json.loads(result)\n",
    "       \n",
    "        # Extract the list of links from the dictionary\n",
    "        ordered_links = result_dict[\"links\"]\n",
    "        return ordered_links\n",
    "\n",
    "def check_job_page_llm(content, openaiSession, model=MODEL_QWEN):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_career_page_finder},\n",
    "            {\"role\": \"user\", \"content\": check_job_page_user_prompt(content)}\n",
    "        ]\n",
    "        response = openaiSession.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip().lower()\n",
    "        print(f\"LLM response: {result}\")\n",
    "        # Check if the response contains \"yes\" or \"no\"\n",
    "        return result \n",
    "\n",
    "def crawl(driver, base_url, url, collected_links, all_seen_links=None, max_depth=2, current_depth=0, openaiSession=None, model=MODEL_LLAMA, found_job_page=None):\n",
    "    # Initialize the all_seen_links set if it's None\n",
    "    if all_seen_links is None:\n",
    "        all_seen_links = set()\n",
    "\n",
    "    # If a job page has already been found, stop further crawling\n",
    "    if found_job_page[0] is not None:\n",
    "        return\n",
    "\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    if url in collected_links:\n",
    "        return\n",
    "    \n",
    "    # Add the current URL to our collected links\n",
    "    collected_links.add(url)\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(1.5, 3.5))  # Human-like pause\n",
    "        \n",
    "        print(f\"Crawling (depth {current_depth}): {url}\")\n",
    "\n",
    "        # Check if the page is a job listings page using the LLM if we are not at the root level\n",
    "        if current_depth != 0:\n",
    "            page_source = driver.page_source\n",
    "\n",
    "            # Check for iframes and append their content to page_source\n",
    "            iframes = driver.find_elements(By.TAG_NAME, \"iframe\")\n",
    "            for iframe in iframes:\n",
    "                try:\n",
    "                    driver.switch_to.frame(iframe)\n",
    "                    page_source += driver.page_source\n",
    "                    driver.switch_to.default_content()\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to extract content from iframe: {e}\")\n",
    "\n",
    "            # Store the page source in a file for debugging or further analysis\n",
    "            with open(\"page_content.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(page_source)\n",
    "                \n",
    "            # Parse the page source with BeautifulSoup\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Remove irrelevant tags to clean up the content\n",
    "            for irrelevant in soup([\"script\", \"style\", \"img\", \"input\", \"meta\", \"noscript\", \"iframe\"]):\n",
    "                irrelevant.decompose()\n",
    "\n",
    "            # Extract visible text from the page\n",
    "            page_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "            # Further clean up the text by removing excessive newlines\n",
    "            page_text = \"\\n\".join(line for line in page_text.splitlines() if line.strip())\n",
    "\n",
    "            # Check if the page is a job listings page using the LLM\n",
    "            if check_job_page_llm(page_text, openaiSession, model):\n",
    "                print(f\"✅ {url} is a job listings page.\")\n",
    "                found_job_page[0] = url  # Set the found job page URL\n",
    "                return\n",
    "            else:\n",
    "                print(f\"❌ {url} is not the job listing page.\")\n",
    "\n",
    "        # Find all <a> links\n",
    "        hrefs = []\n",
    "        links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            # Only include links that:\n",
    "            # 1. Are not None\n",
    "            # 2. Start with the base URL (same domain)\n",
    "            # 3. Haven't been seen before across all pages\n",
    "            if href and href.startswith(base_url) and href not in all_seen_links:  \n",
    "                hrefs.append(href)\n",
    "                # Add to the all_seen_links set to avoid duplicate exploration\n",
    "                all_seen_links.add(href)\n",
    "\n",
    "        print(f\"Found {len(hrefs)} new links in {url}\")\n",
    "        if len(hrefs) > 0:\n",
    "            print(\"New links:\")\n",
    "            for link in hrefs[:3]:  # Only show first 3 to avoid console clutter\n",
    "                print(link) \n",
    "\n",
    "            print(\"Ordered links:\")\n",
    "            if openaiSession:\n",
    "                try:\n",
    "                    ordered_hrefs = order_links_llm(hrefs, openaiSession, model)\n",
    "                    print(\"Ordered links:\")\n",
    "                    for link in ordered_hrefs[:3]:  # Only show first 3\n",
    "                        print(link) \n",
    "                    # Now recursively crawl those hrefs\n",
    "                    for href in ordered_hrefs[:3]:  # Limit to first 3 to avoid excessive crawling\n",
    "                        crawl(driver, base_url, href, collected_links, all_seen_links, max_depth, current_depth + 1, openaiSession, model, found_job_page)\n",
    "                        # Stop further crawling if a job page has been found\n",
    "                        if found_job_page[0] is not None:\n",
    "                            return\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error ordering links: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to crawl {url}: {e}\")\n",
    "\n",
    "def get_career_page(url, openaiSession, model=MODEL_LLAMA):\n",
    "    \"\"\"\n",
    "    Create this WebsiteSelenium object from the given URL using Selenium and BeautifulSoup.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract base URL for domain matching\n",
    "    base_url_parts = url.split('/')\n",
    "    if len(base_url_parts) >= 3:\n",
    "        base_url = base_url_parts[0] + '//' + base_url_parts[2]\n",
    "    else:\n",
    "        base_url = url\n",
    "    \n",
    "    print(f\"Processing website: {url}\")\n",
    "    print(f\"Base domain: {base_url}\")\n",
    "    \n",
    "    # Create a fresh driver for each website\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    # More human-like user agent\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\")\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    \n",
    "    # Try with regular Chrome driver if undetected fails\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = uc.Chrome(options=options)\n",
    "        print(\"Browser initialized successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create undetected_chromedriver: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Configure browser to avoid detection\n",
    "        driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "            \"source\": \"\"\"\n",
    "            Object.defineProperty(navigator, 'webdriver', {\n",
    "                get: () => undefined\n",
    "            })\n",
    "            \"\"\"\n",
    "        })\n",
    "        \n",
    "        collected_links = set()\n",
    "        all_seen_links = set()  # Initialize the set of all seen links\n",
    "        max_depth = 2  # Keep depth low for testing\n",
    "        found_job_page = [None]  # Use a mutable object to track the found job page URL\n",
    "        \n",
    "        # Start crawling\n",
    "        try:\n",
    "            crawl(driver, base_url, url, collected_links, all_seen_links, max_depth, 0, openaiSession, model, found_job_page)\n",
    "            \n",
    "            # Return the found job page URL if any, otherwise return the original URL\n",
    "            return found_job_page[0] if found_job_page[0] is not None else url\n",
    "        except Exception as e:\n",
    "            print(f\"Error during crawling: {e}\")\n",
    "            return url\n",
    "    finally:\n",
    "        try:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "                print(\"Browser closed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error closing driver: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e41729",
   "metadata": {},
   "source": [
    "## Now let's look at the companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c03230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Company:\n",
    "    def __init__(self, company_name, company_url):\n",
    "        \"\"\"\n",
    "        Initialize a Company object with the given name and URL.\n",
    "\n",
    "        Args:\n",
    "            company_name (str): The name of the company.\n",
    "            company_url (str): The URL of the company's website.\n",
    "        \"\"\"\n",
    "        self.company_name = company_name\n",
    "        self.company_url = company_url\n",
    "        self.career_url = None  # This will be populated later\n",
    "        \n",
    "def process_companies_from_csv(file_path, openaiSession, model=MODEL_LLAMA):\n",
    "    \"\"\"\n",
    "    Process companies from a CSV file, extract their career page URLs, and return a list of Company objects.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing company data.\n",
    "        model (str): The model to use for extracting career page URLs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Company objects with career URLs populated.\n",
    "    \"\"\"\n",
    "    companies = []\n",
    "\n",
    "    # Open the CSV file and iterate over its rows\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)  # Use DictReader to access columns by name\n",
    "        for row in reader:\n",
    "            # Create a Company object for each row\n",
    "            company = Company(company_name=row['company_name'], company_url=row['company_url'])\n",
    "            company.career_url = get_career_page(company.company_url, openaiSession, model)  # Get the career page URL\n",
    "            print(f\"Company Name: {company.company_name}, Career URL: {company.career_url}\")\n",
    "            companies.append(company)  # Add the Company object to the companies list\n",
    "\n",
    "    return companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ae36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', openai, MODEL_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158898c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_companies_from_csv('data/sampledomains.csv', nwopenai, MODEL_NW_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process without browser automation\n",
    "process_companies_from_csv('data/sampledomains.csv', openai, MODEL_GPT, use_browser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4356edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing website: https://www.munichre.com\n",
      "Base domain: https://www.munichre.com\n",
      "Browser initialized successfully\n",
      "Crawling (depth 0): https://www.munichre.com\n",
      "Found 72 new links in https://www.munichre.com\n",
      "New links:\n",
      "https://www.munichre.com/en.html\n",
      "https://www.munichre.com/en/risks.html\n",
      "https://www.munichre.com/en/solutions.html\n",
      "Ordered links:\n",
      "ASKING LLM TO ORDER 72 LINKS\n",
      "LLM response: ```json\n",
      "{\n",
      "    \"links\": [\n",
      "        \"https://www.munichre.com/en/careers.html\"\n",
      "    ]\n",
      "}\n",
      "```\n",
      "Error ordering links: Expecting value: line 1 column 1 (char 0)\n",
      "Browser closed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://www.munichre.com'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_career_page('https://www.munichre.com', nwopenai, MODEL_NW_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b16a4923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing website: https://www.munichre.com\n",
      "Base domain: https://www.munichre.com\n",
      "Browser initialized successfully\n",
      "Crawling (depth 0): https://www.munichre.com\n",
      "Found 72 new links in https://www.munichre.com\n",
      "New links:\n",
      "https://www.munichre.com/en.html\n",
      "https://www.munichre.com/en/risks.html\n",
      "https://www.munichre.com/en/solutions.html\n",
      "Ordered links:\n",
      "ASKING LLM TO ORDER 72 LINKS\n",
      "LLM response: {\n",
      "    \"links\": [\n",
      "        \"https://www.munichre.com/en/careers.html\"\n",
      "    ]\n",
      "}\n",
      "Ordered links:\n",
      "https://www.munichre.com/en/careers.html\n",
      "Crawling (depth 1): https://www.munichre.com/en/careers.html\n",
      "LLM response: **no**\n",
      "\n",
      "the page does not explicitly list job openings. it contains general information about working at munich re, but lacks a comprehensive list of current job openings, a job search interface, or any visible job counter indicating available positions.\n",
      "✅ https://www.munichre.com/en/careers.html is a job listings page.\n",
      "Browser closed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://www.munichre.com/en/careers.html'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_career_page('https://www.munichre.com', openai, MODEL_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a64a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careerpageswatchdog-Xx4QWZVo-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
